{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ef88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Добавление дополнительных датасетов\n",
    "df1=pd.read_csv('3rd_checkpoint/train.csv', sep=',')\n",
    "df2=pd.read_csv('3rd_checkpoint/extra_train.csv', sep=',')\n",
    "df3=pd.read_csv('2nd_checkpoint/extra_train.csv', sep=',')\n",
    "df4=pd.read_csv('1st_checkpoint/extra_train.csv', sep=',')\n",
    "df=pd.concat([df1, df2, df3, df4])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b9fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#сохраняем по станциям\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "\n",
    "stations=df['station_id'].unique()\n",
    "for station in stations:\n",
    "    sub_df=df[df['station_id']==station]\n",
    "    print(station)\n",
    "    \n",
    "    sub_df.to_csv('sub_datasets/train_'+str(station)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#перезаписываем с восстановленным временным рядом\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "folder_path='C:/Users/yulas/OneDrive/Документы/ITMO/NSS_lab/emergencyData/sub_datasets'\n",
    "for file in os.listdir(folder_path):\n",
    "    sub_df=pd.read_csv(os.path.join(folder_path, file))\n",
    "    sub_df['date']=pd.to_datetime(sub_df['date'])\n",
    "    sub_df.set_index('date', inplace=True)\n",
    "    idx = pd.date_range(start='1/1/1985', end='31/10/2019', freq='D')\n",
    "    \n",
    "    \n",
    "    \n",
    "    new_df=pd.DataFrame()\n",
    "    cols=['stage_avg', 'stage_min', 'stage_max', 'temp', 'water_code','ice_thickness', 'snow_height', 'place', 'discharge', 'year', 'month', 'day', 'delta_stage_max']\n",
    "    for column in cols:\n",
    "        stage_avg_df=sub_df[column]    \n",
    "        stage_avg_df.index = pd.DatetimeIndex(stage_avg_df.index) \n",
    "        stage_avg_df = stage_avg_df.reindex(idx, fill_value=np.nan)\n",
    "        new_df[column]=stage_avg_df\n",
    "    print(new_df)\n",
    "    new_df['station_id']=sub_df['station_id'][0]\n",
    "    new_df.to_csv('sub_datasets_no_gaps/with_nan/no_gap_'+file, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd4086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#заполняем пропуски федотом (для метеопараметров и временных рядов с недлительными пропусками (меньше полугода))\n",
    "from fedot.core.chains.chain import Chain\n",
    "from fedot.core.chains.node import PrimaryNode, SecondaryNode\n",
    "from fedot.utilities.synth_dataset_generator import generate_synthetic_data\n",
    "from fedot.utilities.ts_gapfilling import ModelGapFiller, SimpleGapFiller\n",
    "\n",
    "folder_path='C:/Users/yulas/OneDrive/Документы/ITMO/NSS_lab/emergencyData/sub_datasets_no_gaps/with_nan'\n",
    "output_folder_path='C:/Users/yulas/OneDrive/Документы/ITMO/NSS_lab/emergencyData/sub_datasets_no_gaps/no_gaps'\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "\n",
    "    new_df_open = pd.read_csv(os.path.join(folder_path, file))\n",
    "    df = pd.read_csv(file_path)\n",
    "    new_df = df\n",
    "    df = df.fillna(9999)\n",
    "    gap_names = ['stage_avg', 'stage_min', 'stage_max']\n",
    "    for gap_name in gap_names:\n",
    "\n",
    "        gap_data=df[gap_name]\n",
    "        print(len(gap_data))\n",
    "\n",
    "        # Filling in gaps using chain from FEDOT\n",
    "        node_lagged = PrimaryNode('lagged')\n",
    "        node_lagged.custom_params = {'window_size': 50}\n",
    "        node_ridge = SecondaryNode('ridge', nodes_from=[node_lagged])\n",
    "        ridge_chain = Chain(node_ridge)\n",
    "        ridge_gapfiller = ModelGapFiller(gap_value=9999, chain=ridge_chain)\n",
    "\n",
    "\n",
    "        without_gap_arr_ridge = ridge_gapfiller.forward_filling(gap_data)\n",
    "        new_df[gap_name]=without_gap_arr_ridge\n",
    "\n",
    "        #plt.plot(np.arange(len(gap_data)), gap_data)\n",
    "        #plt.plot(np.arange(len(gap_data)), without_gap_arr_ridge)\n",
    "        #plt.show()\n",
    "        #print(len(gap_data))\n",
    "\n",
    "    new_df.to_csv(os.path.join(output_folder_path, file), index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa6437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#заполняем пропуски для температуры\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "folder_path='C:/Users/yulas/OneDrive/Документы/ITMO/NSS_lab/emergencyData/2nd_checkpoint/sub_datasets_no_gaps/no_gaps'\n",
    "for file in os.listdir(folder_path):\n",
    "    \n",
    "    df=pd.read_csv(os.path.join(folder_path, file))\n",
    "    df=df.rename(columns={\"Unnamed: 0\": \"date\"})\n",
    "    df['date']=pd.to_datetime(df['date'])\n",
    "   \n",
    "    df[['temp2']] = df[['temp']].fillna(value=0)\n",
    "        \n",
    "    plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "    plt.plot(df['date'], df['temp'])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "    result = seasonal_decompose(df['temp2'], model='additive', period=365)\n",
    "    result.plot()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "    seas=result.seasonal+result.trend.median()\n",
    "    plt.plot(seas)\n",
    "    plt.show()\n",
    "    \n",
    "    df['temp'][df['temp'].isna()]=seas\n",
    "    plt.plot(df['temp'])\n",
    "    plt.show()\n",
    "    \n",
    "    df=df.drop(['temp2'], axis=1)\n",
    "    df.to_csv(os.path.join(folder_path, file), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9146396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#заполняем пропуски для расходов\n",
    "folder_path='C:/Users/yulas/OneDrive/Документы/ITMO/NSS_lab/emergencyData/3rd_checkpoint/sub_datasets_no_gaps/no_gaps'\n",
    "for file in os.listdir(folder_path):\n",
    "    if file in st_with_discharge:\n",
    "        print(file)\n",
    "        df=pd.read_csv(os.path.join(folder_path, file))\n",
    "        df['date']=pd.to_datetime(df['date'])\n",
    "\n",
    "        df[['discharge2']] = df[['discharge']].fillna(value=0)\n",
    "\n",
    "        plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "        plt.plot(df['date'], df['discharge'])\n",
    "        plt.show()\n",
    "\n",
    "        plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "        result = seasonal_decompose(df['discharge2'], model='additive', period=365)\n",
    "        #result.plot()\n",
    "        #plt.show()\n",
    "\n",
    "        plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "        seas=result.seasonal+result.trend.median()\n",
    "        plt.plot(seas)\n",
    "        plt.show()\n",
    "\n",
    "        df['discharge'][df['discharge'].isna()]=seas\n",
    "        plt.plot(df['discharge'])\n",
    "        plt.show()\n",
    "\n",
    "        df=df.drop(['discharge2'], axis=1)\n",
    "        df.to_csv(os.path.join(folder_path, file), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
